
<html>

<head>
	<title>Humpty Dumpty</title>
</head>


<body>

	<center>
		<br><br><br>

		<video id="videoID" height="260" controls>
			<source src="http://videoproject.stanford.edu/videos/IDed/V26.mp4">
			Your browser does not support HTML5 video. Please update your browser.
		</video>

	</center>

	<script src="https://download.affectiva.com/js/3.2.1/affdex.js"/>
	<script>
		/*
		   affdex.FaceDetectorMode.LARGE_FACES=Faces occupying large portions of the frame
		   affdex.FaceDetectorMode.SMALL_FACES=Faces occupying small portions of the frame
		*/
		var faceMode = affdex.FaceDetectorMode.SMALL_FACES;

		// Construct a FrameDetector and specify the image width / height and face detector mode.
		var detector = new affdex.FrameDetector(faceMode);

		detector.addEventListener("onInitializeSuccess", function() {
			console.log("Initialized face tracker.");
		});

		detector.addEventListener("onInitializeFailure", function() {
			console.log("Failed to initialize face tracker.");
		});

		/* 
		  onImageResults success is called when a frame is processed successfully and receives 3 parameters:
		  - Faces: Dictionary of faces in the frame keyed by the face id.
		           For each face id, the values of detected emotions, expressions, appearane metrics 
		           and coordinates of the feature points
		  - image: An imageData object containing the pixel values for the processed frame.
		  - timestamp: The timestamp of the captured image in seconds.
		*/
		detector.addEventListener("onImageResultsSuccess", function (faces, image, timestamp) {
			console.log("Success!");
		});

		/* 
		  onImageResultsFailure receives 3 parameters:
		  - image: An imageData object containing the pixel values for the processed frame.
		  - timestamp: An imageData object contain the pixel values for the processed frame.
		  - err_detail: A string contains the encountered exception.
		*/
		detector.addEventListener("onImageResultsFailure", function (image, timestamp, err_detail) {});

		detector.addEventListener("onResetSuccess", function() {});

		detector.addEventListener("onResetFailure", function() {});

		detector.addEventListener("onStopSuccess", function() {});

		detector.addEventListener("onStopFailure", function() {});

		detector.detectAllExpressions();
		detector.detectAllEmotions();
		//detector.detectAllEmojis();
		detector.detectAllAppearance();

		detector.start();

		//Get a canvas element from DOM
		var aCanvas = document.getElementById("videoID");
		var context = aCanvas.getContext('2d');

		//Cache the timestamp of the first frame processed
		var startTimestamp = (new Date()).getTime() / 1000;

		//Get imageData object.
		var imageData = context.getImageData(0, 0, 640, 480);

		//Get current time in seconds
		var now = (new Date()).getTime() / 1000;

		//Get delta time between the first frame and the current frame.
		var deltaTime = now - startTimestamp;

		//Process the frame
		detector.process(imageData, deltaTime);

	</script>

</body>


</html>